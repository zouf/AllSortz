We evaluated our technique in phases. First, we generated fake data that validates the correctness of the model, then we evaluated a small portion of the dataset, and finally evaluated on the full dataset.\subsection{Generated Data}The core algorithm relies on finding a P and Q matrix that relate users and businesses to latent factors (the columns of P and Q). Figuring out specifically how a user (U) feels about a business(B) is just a matter of taking the dot product of P[U] and Q[B]. Therefore, we started evaluation by randomly generating a P and Q matrix, and then creating the rating table based on the matrices. Running our algorithm on this data yielded similar P and Q matrices, which indicated that the algorithm was (fundamentally) doing whatwe expected it to do.\subsection{Real Data}However, using generated data says nothing about the validity of our model. In order to show just how effective our model would be, we needed to evaluate on the actualbusinesses data from Yelp. We started evaluation by taking dense chunks of data from the Yelp set. Thus, we'd only have a couple of thousand ratings (instead of hundreds of thousands). This step was critical to expose inefficiencies in our calculations (indeed, once we started doing this, we immediately realized the need to switch our code from Python to a C++ module).Real evaluation ... TO BE DETERMINED BASED ON WHAT EXPERIMENTS WE RUN!See Figure 1,2,3,4...Normalization vs. NOT normalization\subsection{Interpreting Results}For our run of the data that included all businesses and users that had ratings in the Yelp dataset, we achieved an average RSS of \bestRSS using 5-fold cross validation. This means our best RMSE was \bestRMSE. Comparatively, the winners of the Netflix prize used a combination of dozens of predictors. However, their best single prediction achieved an average RMSE of \bestNetflixRMSE (\cite{netprize}).We felt pleased being able to achieve an RMSE so close to an algorithm that won such a prestigious prize. What's particularly interesting about our result is that it shows predictions across a wide variety of businesses-genres can be useful in predicting interests in unrelated genres. For example, predictions for clothes shoppings can be used to help predict where you might like to go out to eat. This indicates that there is a deeper connection to how people feel about businesses than just their genres. Evidence for this hypothesis can be seen in Figure (INCLUDE FIGURE OF SOME LATENT FACTORS... just excerpts of text will be fine). Notice that there is no obvious relationship relationship between the top businesses found in most of the latent factors. This contrasts Prof. Blei's class demo for movies where the latent factors for movies were roughly associated with the movie genre (e.g. Terminator, Star Wars, and Bladerunner are lumped together). In future work, we hope to explore the relationships we've found amongst businesses in more detail.